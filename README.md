# ğŸš€ PROJECT 2B: GitHub Repository Topic Classification Based on Textual Metadata  
# Dá»± Ã¡n 2B: PhÃ¢n loáº¡i chá»§ Ä‘á» kho GitHub dá»±a trÃªn siÃªu dá»¯ liá»‡u vÄƒn báº£n  

**ğŸ§  Technologies:** Python Â· PyTorch Â· HuggingFace Transformers Â· PEFT (LoRA) Â· Scikit-learn Â· NLTK Â· GitHub GraphQL API  

---

## ğŸ§© 1. Introduction & Goal  
## Giá»›i thiá»‡u vÃ  Má»¥c tiÃªu Dá»± Ã¡n  

**Project Name:** PROJECT 2B: GitHub Repository Topic Classification Based on Textual Metadata.  
**TÃªn Dá»± Ã¡n:** PROJECT 2B: PhÃ¢n loáº¡i chá»§ Ä‘á» kho GitHub dá»±a trÃªn siÃªu dá»¯ liá»‡u vÄƒn báº£n.  

**Problem:** The rapid growth of open-source repositories on GitHub has produced vast textual metadata. However, user-generated topic tags are often **inconsistent, incomplete, and unreliable**.  
**Váº¥n Ä‘á»:** Sá»± phÃ¡t triá»ƒn nhanh cá»§a kho mÃ£ nguá»“n má»Ÿ GitHub táº¡o ra lÆ°á»£ng lá»›n siÃªu dá»¯ liá»‡u vÄƒn báº£n, nhÆ°ng nhÃ£n chá»§ Ä‘á» thÆ°á»ng **khÃ´ng nháº¥t quÃ¡n, khÃ´ng Ä‘áº§y Ä‘á»§ vÃ  phá»¥ thuá»™c ngÆ°á»i dÃ¹ng**.  

**Goal:** Develop an **automated and robust classification approach** based on textual metadata, primarily from **`README.md`** files.  
**Má»¥c tiÃªu:** PhÃ¡t triá»ƒn phÆ°Æ¡ng phÃ¡p **phÃ¢n loáº¡i tá»± Ä‘á»™ng vÃ  máº¡nh máº½** dá»±a trÃªn siÃªu dá»¯ liá»‡u vÄƒn báº£n, chá»§ yáº¿u tá»« file **`README.md`**.  

**Main Contributions:**  
**ÄÃ³ng gÃ³p chÃ­nh:**  
1ï¸âƒ£ Construction of a **large, diverse public dataset** with over **50 distinct topics** from README.md files.  
1ï¸âƒ£ XÃ¢y dá»±ng vÃ  cÃ´ng khai **táº­p dá»¯ liá»‡u lá»›n, Ä‘a dáº¡ng** gá»“m hÆ¡n **50 chá»§ Ä‘á»** khÃ¡c nhau tá»« README.md.  
2ï¸âƒ£ Development of a **domain-specific text preprocessing pipeline**, enriching stopwords with software-related tokens.  
2ï¸âƒ£ PhÃ¡t triá»ƒn **pipeline tiá»n xá»­ lÃ½ Ä‘áº·c thÃ¹ miá»n**, má»Ÿ rá»™ng stopword báº±ng cÃ¡c tá»« liÃªn quan Ä‘áº¿n láº­p trÃ¬nh.  
3ï¸âƒ£ Application of **Transformer-based models using PEFT (LoRA)** for high accuracy and computational efficiency.  
3ï¸âƒ£ Ãp dá»¥ng **mÃ´ hÃ¬nh Transformer vá»›i PEFT (LoRA)** nháº±m Ä‘áº¡t hiá»‡u suáº¥t cao vÃ  tiáº¿t kiá»‡m tÃ i nguyÃªn.  

---

## ğŸ’¾ 2. Data & Preprocessing  
## Dá»¯ liá»‡u vÃ  Tiá»n xá»­ lÃ½  

### A. Data Collection  
### A. Thu tháº­p Dá»¯ liá»‡u  

**Source:** Collected via **GitHub GraphQL API**.  
**Nguá»“n:** Thu tháº­p qua **GitHub GraphQL API**.  

**Scale:** Contains **57,368 README.md files**.  
**Quy mÃ´:** Gá»“m **57.368 file README.md**.  

**Topic Coverage:** Over **50 distinct IT-related topics**.  
**Pháº¡m vi Chá»§ Ä‘á»:** HÆ¡n **50 chá»§ Ä‘á» trong lÄ©nh vá»±c CNTT**.  

**Sampling Strategy (Diversity Ensured):** For each topic, up to 2,000 repositories were selected based on:  
**Chiáº¿n lÆ°á»£c láº¥y máº«u (Ä‘áº£m báº£o Ä‘a dáº¡ng):** Má»—i chá»§ Ä‘á» thu tháº­p tá»‘i Ä‘a 2.000 kho theo cÃ¡c tiÃªu chÃ­ sau:  
1ï¸âƒ£ Most Starred  
1ï¸âƒ£ ÄÆ°á»£c gáº¯n sao nhiá»u nháº¥t  
2ï¸âƒ£ Most Forked  
2ï¸âƒ£ ÄÆ°á»£c fork nhiá»u nháº¥t  
3ï¸âƒ£ Recently Updated  
3ï¸âƒ£ Cáº­p nháº­t gáº§n Ä‘Ã¢y nháº¥t  
4ï¸âƒ£ Best Match (random sampling)  
4ï¸âƒ£ Máº«u ngáº«u nhiÃªn Ä‘á»ƒ tÄƒng tÃ­nh Ä‘a dáº¡ng  

**Target Labels:** 50 topics mapped into **10 broader IT categories** for multi-class classification.  
**PhÃ¢n loáº¡i Má»¥c tiÃªu:** 50 chá»§ Ä‘á» Ã¡nh xáº¡ thÃ nh **10 danh má»¥c IT chÃ­nh** cho bÃ i toÃ¡n Ä‘a lá»›p.  

**Data Split:** **80% train / 20% test** using stratified sampling (45,894 train, 11,474 test).  
**PhÃ¢n phá»‘i Dá»¯ liá»‡u:** **80% huáº¥n luyá»‡n / 20% kiá»ƒm tra** báº±ng láº¥y máº«u phÃ¢n táº§ng.  

---

### B. Preprocessing Pipeline  
### B. Quy trÃ¬nh Tiá»n xá»­ lÃ½  

1ï¸âƒ£ **Remove Code & URLs:** Clean Markdown code blocks, inline code, and hyperlinks.  
1ï¸âƒ£ **XÃ³a Code & URL:** Loáº¡i bá» cÃ¡c khá»‘i code, mÃ£ ná»™i tuyáº¿n, liÃªn káº¿t.  

2ï¸âƒ£ **Normalize Markdown Syntax:** Convert formatting to plain text.  
2ï¸âƒ£ **Chuáº©n hÃ³a cÃº phÃ¡p Markdown:** Chuyá»ƒn Ä‘á»‹nh dáº¡ng vá» vÄƒn báº£n thuáº§n.  

3ï¸âƒ£ **Tokenization & Lemmatization:** Applied via `NLTK`.  
3ï¸âƒ£ **Tokenization & Lemmatization:** Thá»±c hiá»‡n báº±ng `NLTK`.  

4ï¸âƒ£ **Custom Stopwords:** Extend with domain-specific tokens (*install, build, repository, file*...).  
4ï¸âƒ£ **Stopword má»Ÿ rá»™ng:** Bá»• sung cÃ¡c tá»« ká»¹ thuáº­t (*install, build, repository, file*...).  

5ï¸âƒ£ **Lowercasing & Noise Removal:** Remove digits, punctuation, special symbols.  
5ï¸âƒ£ **Chuyá»ƒn chá»¯ thÆ°á»ng & loáº¡i nhiá»…u:** XÃ³a kÃ½ tá»± Ä‘áº·c biá»‡t, sá»‘, dáº¥u cÃ¢u.  

---

## âš™ï¸ 3. Methodology & Models  
## PhÆ°Æ¡ng phÃ¡p vÃ  Kiáº¿n trÃºc MÃ´ hÃ¬nh  

### A. Feature Representation  
### A. Biá»ƒu diá»…n Äáº·c trÆ°ng  

**Classical Models:** Use **Sentence-BERT embeddings (all-MiniLM-L6-v2)** â†’ 384-dimensional vectors.  
**MÃ´ hÃ¬nh cá»• Ä‘iá»ƒn:** DÃ¹ng **Sentence-BERT (all-MiniLM-L6-v2)** â†’ vector Ä‘áº·c trÆ°ng 384 chiá»u.  

**Transformer Models:** Tokenized using **AutoTokenizer (Mistral-7B-v0.1)** with fixed length **512 tokens**.  
**MÃ´ hÃ¬nh Transformer:** Sá»­ dá»¥ng **AutoTokenizer (Mistral-7B-v0.1)** vá»›i chuá»—i **512 tokens** cá»‘ Ä‘á»‹nh.  

---

### B. Models Compared  
### B. MÃ´ hÃ¬nh ÄÆ°á»£c So sÃ¡nh  

**Classical Machine Learning Models:** (trained on MiniLM embeddings)  
**MÃ´ hÃ¬nh Há»c mÃ¡y Cá»• Ä‘iá»ƒn:** (huáº¥n luyá»‡n trÃªn nhÃºng MiniLM)  
- Logistic Regression (LR)  
- Random Forest (RF)  
- Support Vector Classifier (SVC)  
- K-Nearest Neighbors (KNN)  

**Modern Transformer (PEFT):**  
**MÃ´ hÃ¬nh Transformer Hiá»‡n Ä‘áº¡i (PEFT):**  
- **Mistral-7B** fine-tuned with **Low-Rank Adaptation (LoRA)** and **4-bit quantization**.  
- Reduces trainable parameters ($r=16$) while preserving performance.  
- **Mistral-7B** tinh chá»‰nh vá»›i **LoRA** vÃ  **lÆ°á»£ng tá»­ hÃ³a 4-bit**, giáº£m tham sá»‘ mÃ  váº«n giá»¯ hiá»‡u nÄƒng.  

---

## ğŸ“Š 4. Experimental Results  
## Káº¿t quáº£ Thá»±c nghiá»‡m  

**Metrics:** Precision (P), Recall (R), F1-score (F1), Accuracy (A).  
**Chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡:** Precision (P), Recall (R), F1-score (F1), Accuracy (A).  

### A. Classical Models Performance  
### A. Hiá»‡u suáº¥t MÃ´ hÃ¬nh Cá»• Ä‘iá»ƒn  

| Model | Precision | Recall | **F1-Score** |  
| :--- | :--- | :--- | :--- |  
| Logistic Regression | 0.66 | 0.69 | 0.66 |  
| Random Forest | 0.58 | 0.62 | **0.56** (Lowest) |  
| SVC | 0.66 | 0.69 | 0.67 |  
| **KNN** | 0.67 | 0.70 | **0.68** (Best) |  

**Observation:** Classical models capture basic text features but fail to represent complex semantics.  
**Nháº­n xÃ©t:** MÃ´ hÃ¬nh cá»• Ä‘iá»ƒn náº¯m báº¯t Ä‘áº·c trÆ°ng cÆ¡ báº£n nhÆ°ng chÆ°a thá»ƒ hiá»‡n Ä‘Æ°á»£c ngá»¯ nghÄ©a phá»©c táº¡p.  

---

### B. Transformer Model (Mistral-7B + PEFT/LoRA)  
### B. MÃ´ hÃ¬nh Transformer (Mistral-7B + PEFT/LoRA)  

| Metric Type | Precision | Recall | **F1-score** | **Accuracy** |  
| :--- | :--- | :--- | :--- | :--- |  
| Per-class Range | 0.94â€“0.97 | 0.92â€“0.97 | 0.93â€“0.96 | â€“ |  
| **Macro / Weighted Avg.** | **0.95** | **0.95** | **0.95** | **0.95** |  

**Observation:** Mistral-7B (PEFT/LoRA) achieved **state-of-the-art performance**, with **consistent accuracy of 0.95** across all classes.  
**Nháº­n xÃ©t:** Mistral-7B (PEFT/LoRA) Ä‘áº¡t **hiá»‡u suáº¥t vÆ°á»£t trá»™i**, vá»›i **Ä‘á»™ chÃ­nh xÃ¡c 0.95** á»•n Ä‘á»‹nh trÃªn má»i lá»›p.  

---

## ğŸ§  5. Conclusion  
## Káº¿t luáº­n  

âœ… **Transformer Strength:** Fine-tuned Mistral-7B demonstrates superior contextual understanding compared to classical ML.  
âœ… **Sá»©c máº¡nh cá»§a Transformer:** Mistral-7B tinh chá»‰nh thá»ƒ hiá»‡n kháº£ nÄƒng hiá»ƒu ngá»¯ cáº£nh vÆ°á»£t trá»™i so vá»›i mÃ´ hÃ¬nh cá»• Ä‘iá»ƒn.  

âš™ï¸ **Efficiency of PEFT:** LoRA and 4-bit quantization enable **high accuracy with minimal computation cost**, suitable for large-scale repository analysis.  
âš™ï¸ **Hiá»‡u quáº£ cá»§a PEFT:** LoRA vÃ  lÆ°á»£ng tá»­ hÃ³a 4-bit mang láº¡i **hiá»‡u suáº¥t cao, chi phÃ­ tháº¥p**, phÃ¹ há»£p cho bÃ i toÃ¡n quy mÃ´ lá»›n.  

---

## ğŸ“š Analogy  
## So sÃ¡nh Minh há»a  

Classical models (LR, KNN) resemble sorting books by keywords â€” effective for simple topics but weak for nuanced meaning.  
MÃ´ hÃ¬nh cá»• Ä‘iá»ƒn giá»‘ng viá»‡c phÃ¢n loáº¡i sÃ¡ch báº±ng tá»« khÃ³a â€” hiá»‡u quáº£ vá»›i chá»§ Ä‘á» Ä‘Æ¡n giáº£n nhÆ°ng kÃ©m vá»›i ngá»¯ cáº£nh phá»©c táº¡p.  

Fine-tuned Transformers like **Mistral-7B + PEFT** act as **expert librarians** who truly understand context and semantics, achieving precise categorization even in ambiguous cases.  
Transformer tinh chá»‰nh nhÆ° **Mistral-7B + PEFT** giá»‘ng **thá»§ thÆ° chuyÃªn nghiá»‡p**, hiá»ƒu sÃ¢u ngá»¯ nghÄ©a vÃ  phÃ¢n loáº¡i chÃ­nh xÃ¡c cáº£ trÆ°á»ng há»£p mÆ¡ há»“.  
