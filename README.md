# ğŸ”¬ GitHub Repository Topic Classification Based on Textual Metadata
*(PhÃ¢n loáº¡i chá»§ Ä‘á» kho GitHub dá»±a trÃªn siÃªu dá»¯ liá»‡u vÄƒn báº£n)*

![Python](https://img.shields.io/badge/Python-3.10.11-blue?logo=python)
![Transformers](https://img.shields.io/badge/Transformers-Models-yellow?logo=huggingface)
![PyTorch](https://img.shields.io/badge/PyTorch-Training-red?logo=pytorch)
![PEFT](https://img.shields.io/badge/PEFT-Fine--tuning-blueviolet?logo=huggingface)
![NLTK](https://img.shields.io/badge/NLTK-Preprocessing-green?logo=nltk)
![Scikit-learn](https://img.shields.io/badge/Scikit--learn-ML%20Models-orange?logo=scikit-learn)
![GraphQL](https://img.shields.io/badge/GraphQL-API-E10098?logo=graphql)

---

## ğŸ¯ Introduction & Goal *(Giá»›i thiá»‡u & Má»¥c tiÃªu)*

**Problem:** The rapid growth of open-source (OSS) repositories on GitHub has generated vast amounts of textual metadata. While GitHub allows users to assign topics, these labels are often **inconsistent, incomplete, and user-generated**. <br>
*(Váº¥n Ä‘á»: Sá»± phÃ¡t triá»ƒn nhanh chÃ³ng cá»§a cÃ¡c kho mÃ£ nguá»“n má»Ÿ (OSS) trÃªn GitHub Ä‘Ã£ táº¡o ra lÆ°á»£ng lá»›n siÃªu dá»¯ liá»‡u vÄƒn báº£n. Máº·c dÃ¹ GitHub cho phÃ©p ngÆ°á»i dÃ¹ng gÃ¡n chá»§ Ä‘á», cÃ¡c nhÃ£n nÃ y thÆ°á»ng khÃ´ng nháº¥t quÃ¡n, khÃ´ng Ä‘áº§y Ä‘á»§ vÃ  do ngÆ°á»i dÃ¹ng táº¡o.)*

**Goal:** To develop an automated and comprehensive approach to classify repositories into specific topics based on their textual metadata, primarily the content from **`README.md`** files.<br>
*(Má»¥c tiÃªu: PhÃ¡t triá»ƒn má»™t cÃ¡ch tiáº¿p cáº­n tá»± Ä‘á»™ng vÃ  toÃ n diá»‡n Ä‘á»ƒ phÃ¢n loáº¡i cÃ¡c kho lÆ°u trá»¯ vÃ o cÃ¡c chá»§ Ä‘á» cá»¥ thá»ƒ dá»±a trÃªn siÃªu dá»¯ liá»‡u vÄƒn báº£n, chá»§ yáº¿u lÃ  ná»™i dung tá»« file **`README.md`**.)*

**Main Contributions:** *(ÄÃ³ng gÃ³p chÃ­nh:)*
    
* Constructed and publicly described a **large, diverse dataset** of `README.md` files, covering over **fifty different topics**.<br>
*(XÃ¢y dá»±ng vÃ  mÃ´ táº£ cÃ´ng khai má»™t **táº­p dá»¯ liá»‡u lá»›n, Ä‘a dáº¡ng** gá»“m cÃ¡c file `README.md`, bao gá»“m hÆ¡n **nÄƒm mÆ°Æ¡i chá»§ Ä‘á»** khÃ¡c nhau.)*

*  Developed a **robust text preprocessing** pipeline tailored for software documentation (e.g., stopword enrichment with domain-specific tokens).<br>
*(PhÃ¡t triá»ƒn quy trÃ¬nh **tiá»n xá»­ lÃ½ vÄƒn báº£n máº¡nh máº½** Ä‘Æ°á»£c Ä‘iá»u chá»‰nh cho tÃ i liá»‡u pháº§n má»m (vÃ­ dá»¥: lÃ m giÃ u stopword báº±ng cÃ¡c mÃ£ thÃ´ng bÃ¡o dÃ nh riÃªng cho miá»n).)*

* Applied and evaluated state-of-the-art Transformer-based models, leveraging **PEFT (LoRA)** to achieve high performance and computational efficiency. <br>
*(Ãp dá»¥ng vÃ  Ä‘Ã¡nh giÃ¡ cÃ¡c mÃ´ hÃ¬nh dá»±a trÃªn Transformer hiá»‡n Ä‘áº¡i, táº­n dá»¥ng **PEFT (LoRA)** Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t cao vÃ  hiá»‡u quáº£ tÃ­nh toÃ¡n.)*

---

## ğŸ“Š Data & Preprocessing *(Dá»¯ liá»‡u & Tiá»n xá»­ lÃ½)*

### A. Data Collection *(Thu tháº­p Dá»¯ liá»‡u)*

* **Source:** Used the **GitHub GraphQL API** for data collection.
    *(**Nguá»“n:** Sá»­ dá»¥ng **GitHub GraphQL API** Ä‘á»ƒ thu tháº­p dá»¯ liá»‡u.)*
* **Scale:** The final dataset contains **57,368 `README.md` files**.
    *(**Quy mÃ´:** Táº­p dá»¯ liá»‡u cuá»‘i cÃ¹ng chá»©a **57.368 file `README.md`**.)*
* **Topic Scope:** Over 50 distinct topics within the Information Technology (IT) domain.
    *(**Pháº¡m vi Chá»§ Ä‘á»:** HÆ¡n 50 chá»§ Ä‘á» riÃªng biá»‡t trong lÄ©nh vá»±c CÃ´ng nghá»‡ ThÃ´ng tin (IT).)*
* **Sampling Strategy (to ensure diversity):** For each topic, up to 2,000 repositories were collected based on 4 complementary criteria (approx. 500 repos each):
    *(**Chiáº¿n lÆ°á»£c láº¥y máº«u (Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh Ä‘a dáº¡ng):** Äá»‘i vá»›i má»—i chá»§ Ä‘á», thu tháº­p tá»›i 2.000 kho lÆ°u trá»¯ dá»±a trÃªn 4 tiÃªu chÃ­ bá»• sung (má»—i tiÃªu chÃ­ khoáº£ng 500 repo):)*
    1.  **Most Starred**
        *(**Most Starred** (ÄÆ°á»£c gáº¯n sao nhiá»u nháº¥t).)*
    2.  **Most Forked**
        *(**Most Forked** (ÄÆ°á»£c fork nhiá»u nháº¥t).)*
    3.  **Recently Updated**
        *(**Recently Updated** (ÄÆ°á»£c cáº­p nháº­t gáº§n Ä‘Ã¢y).)*
    4.  **Best Match** (Random sampling for diversity)
        *(**Best Match** (Máº«u ngáº«u nhiÃªn Ä‘á»ƒ tÄƒng tÃ­nh Ä‘a dáº¡ng).)*
* **Target Labels:** 50 granular topics were mapped to **10 broader categories** representing major IT domains, serving as the target labels for the multi-class classification task. <br>
    *(**PhÃ¢n loáº¡i Má»¥c tiÃªu (Target Labels):** 50 chá»§ Ä‘á» chi tiáº¿t Ä‘Ã£ Ä‘Æ°á»£c Ã¡nh xáº¡ thÃ nh **10 danh má»¥c rá»™ng hÆ¡n** (categories) Ä‘áº¡i diá»‡n cho cÃ¡c miá»n chÃ­nh trong IT, dÃ¹ng lÃ m nhÃ£n má»¥c tiÃªu cho nhiá»‡m vá»¥ phÃ¢n loáº¡i Ä‘a lá»›p.)*
* **Data Distribution:** The dataset was split using **stratified sampling** with an **80% Train** and **20% Test** ratio. Total 57,368 samples, with 45,894 training and 11,474 test samples. <br>
    *(**PhÃ¢n phá»‘i Dá»¯ liá»‡u:** Táº­p dá»¯ liá»‡u Ä‘Æ°á»£c phÃ¢n chia theo **láº¥y máº«u phÃ¢n táº§ng (stratified sampling)** vá»›i tá»· lá»‡ **80% cho táº­p huáº¥n luyá»‡n (Train)** vÃ  **20% cho táº­p kiá»ƒm tra (Test)**. Tá»•ng cá»™ng 57.368 máº«u, vá»›i 45.894 máº«u huáº¥n luyá»‡n vÃ  11.474 máº«u kiá»ƒm tra.)*

### B. Preprocessing Pipeline *(Quy trÃ¬nh Tiá»n xá»­ lÃ½)*

A custom preprocessing pipeline was applied to remove noise typical in software documentation:
*(Má»™t pipeline tiá»n xá»­ lÃ½ tÃ¹y chá»‰nh Ä‘Ã£ Ä‘Æ°á»£c Ã¡p dá»¥ng Ä‘á»ƒ loáº¡i bá» nhiá»…u Ä‘iá»ƒn hÃ¬nh trong tÃ i liá»‡u pháº§n má»m:)*

1.  **Code and URL Removal:** Stripped Markdown code blocks, inline code snippets, and hyperlinks. <br>
    *(**Loáº¡i bá» Code vÃ  URL:** XÃ³a cÃ¡c khá»‘i mÃ£ Markdown, Ä‘oáº¡n mÃ£ ná»™i tuyáº¿n vÃ  siÃªu liÃªn káº¿t.)*
2.  **Markdown Syntax Cleaning:** Normalized headers, bold/italic formatting to plain text.<br>
    *(**LÃ m sáº¡ch CÃº phÃ¡p Markdown:** Chuáº©n hÃ³a cÃ¡c dáº¥u tiÃªu Ä‘á», Ä‘á»‹nh dáº¡ng in Ä‘áº­m/in nghiÃªng thÃ nh vÄƒn báº£n thuáº§n tÃºY.)*
3.  **Tokenization and Lemmatization:** Used `NLTK` to reduce word inflections.<br>
    *(**Tokenization vÃ  Lemmatization:** Sá»­ dá»¥ng `NLTK` Ä‘á»ƒ giáº£m cÃ¡c dáº¡ng biáº¿n tá»‘ cá»§a tá»«.)*
4.  **Stopword Filtering:** Extended the standard English stopword list with a custom list of programming-related terms (e.g., *install, build, repository, command, example, file*) to reduce uninformative content.<br>
    *(**Lá»c Stopword:** Má»Ÿ rá»™ng danh sÃ¡ch stopword tiáº¿ng Anh tiÃªu chuáº©n báº±ng má»™t danh sÃ¡ch cÃ¡c thuáº­t ngá»¯ liÃªn quan Ä‘áº¿n láº­p trÃ¬nh (vÃ­ dá»¥: *install, build, repository, command, example, file*) Ä‘á»ƒ giáº£m thiá»ƒu ná»™i dung khÃ´ng mang thÃ´ng tin.)*
5.  **Lowercasing and Noise Removal:** All text was lowercased and cleaned of punctuation, digits, and special characters.<br>
    *(**Chuyá»ƒn vá» chá»¯ thÆ°á»ng vÃ  loáº¡i bá» nhiá»…u:** Táº¥t cáº£ vÄƒn báº£n Ä‘Æ°á»£c chuyá»ƒn vá» chá»¯ thÆ°á»ng vÃ  lÃ m sáº¡ch khá»i dáº¥u cÃ¢u, chá»¯ sá»‘ vÃ  kÃ½ tá»± Ä‘áº·c biá»‡t.)*

---

## ğŸ› ï¸ Methodology & Models *(PhÆ°Æ¡ng phÃ¡p & Kiáº¿n trÃºc MÃ´ hÃ¬nh)*

### A. Feature Representation *(Biá»ƒu diá»…n Äáº·c trÆ°ng)*


* **For Classical Models:** Utilized **Sentence-BERT** embeddings (specifically the **all-MiniLM-L6-v2** model) to convert each preprocessed README into a dense **384-dimensional** feature vector.<br>
    *(**Äá»‘i vá»›i MÃ´ hÃ¬nh Cá»• Ä‘iá»ƒn:** Sá»­ dá»¥ng nhÃºng cÃ¢u **Sentence-BERT** (cá»¥ thá»ƒ lÃ  mÃ´ hÃ¬nh **all-MiniLM-L6-v2**) Ä‘á»ƒ chuyá»ƒn Ä‘á»•i má»—i file README Ä‘Ã£ Ä‘Æ°á»£c tiá»n xá»­ lÃ½ thÃ nh má»™t vector Ä‘áº·c trÆ°ng dÃ y Ä‘áº·c **384 chiá»u**.)* 
* **For Transformer Models:** Used the **AutoTokenizer** corresponding to the base model **Mistral-7B-v0.1**. Each text was encoded into a fixed-length sequence of **512 tokens**.<br>
    *(**Äá»‘i vá»›i MÃ´ hÃ¬nh Transformer:** Sá»­ dá»¥ng **AutoTokenizer** tÆ°Æ¡ng á»©ng vá»›i mÃ´ hÃ¬nh cÆ¡ sá»Ÿ **Mistral-7B-v0.1**. Má»—i vÄƒn báº£n Ä‘Æ°á»£c mÃ£ hÃ³a thÃ nh má»™t chuá»—i ID token cÃ³ Ä‘á»™ dÃ i cá»‘ Ä‘á»‹nh **512 tokens**.)*

### B. Models Compared *(MÃ´ hÃ¬nh Ä‘Æ°á»£c So sÃ¡nh)*

1.  **Classical Machine Learning Models:** Trained on MiniLM embeddings:<br>
    *(**MÃ´ hÃ¬nh Há»c mÃ¡y Cá»• Ä‘iá»ƒn:** Huáº¥n luyá»‡n trÃªn cÃ¡c nhÃºng MiniLM:)*
    * Logistic Regression (LR)
    * Random Forest (RF)
    * Support Vector Classifier (SVC) (using a linear kernel)
    * K-Nearest Neighbors (KNN)
2.  **Modern Transformer Model (PEFT):**<br>
    *(**MÃ´ hÃ¬nh Transformer Hiá»‡n Ä‘áº¡i (PEFT):**)*
    * **Mistral-7B** fine-tuned using **Low-Rank Adaptation (LoRA)** and **4-bit Quantization**.<br>
        *(**Mistral-7B** Ä‘Æ°á»£c tinh chá»‰nh báº±ng cÃ¡ch sá»­ dá»¥ng **Low-Rank Adaptation (LoRA)** vÃ  **LÆ°á»£ng tá»­ hÃ³a 4-bit**.)*
    * The PEFT/LoRA technique significantly reduced trainable parameters (using a low rank $r=16$) while maintaining robust model performance.<br>
        *(Ká»¹ thuáº­t PEFT/LoRA giáº£m Ä‘Ã¡ng ká»ƒ cÃ¡c tham sá»‘ huáº¥n luyá»‡n (sá»­ dá»¥ng chiá»u rank tháº¥p $r=16$) trong khi váº«n duy trÃ¬ hiá»‡u suáº¥t mÃ´ hÃ¬nh máº¡nh máº½.)*

---

## ğŸ“ˆ Experimental Results *(Káº¿t quáº£ Thá»±c nghiá»‡m)*

Performance was evaluated using Precision (P), Recall (R), F1-score (F1), and Accuracy.<br>
*(Hiá»‡u suáº¥t Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ báº±ng Precision (P), Recall (R), F1-score (F1), vÃ  Accuracy.)*

### A. Classical Model Performance *(Hiá»‡u suáº¥t MÃ´ hÃ¬nh Cá»• Ä‘iá»ƒn)*

The classical machine learning models achieved **moderate results**, indicating a limited ability to capture complex semantic context.<br>
*(CÃ¡c mÃ´ hÃ¬nh há»c mÃ¡y cá»• Ä‘iá»ƒn Ä‘áº¡t káº¿t quáº£ **á»Ÿ má»©c vá»«a pháº£i**, cho tháº¥y kháº£ nÄƒng háº¡n cháº¿ trong viá»‡c náº¯m báº¯t bá»‘i cáº£nh ngá»¯ nghÄ©a phá»©c táº¡p.)*

| Model | Precision | Recall | **F1-Score** |
| :--- | :--- | :--- | :--- |
| Logistic Regression | 0.66 | 0.69 | 0.66 |
| Random Forest | 0.58 | 0.62 | **0.56** (Lowest) |
| SVC | 0.66 | 0.69 | 0.67 |
| **KNN** | 0.67 | 0.70 | **0.68** (Highest in classical) |

### B. Transformer Model Performance (Mistral-7B + PEFT/LoRA) *(Hiá»‡u suáº¥t MÃ´ hÃ¬nh Transformer (Mistral-7B + PEFT/LoRA))*

The fine-tuned Transformer model achieved **superior performance improvements**. <br>
*(MÃ´ hÃ¬nh Transformer Ä‘Æ°á»£c tinh chá»‰nh Ä‘áº¡t Ä‘Æ°á»£c sá»± **cáº£i thiá»‡n hiá»‡u suáº¥t vÆ°á»£t trá»™i**.)*

| Metric Type | Precision | Recall | **F1-score** | **Accuracy** |
| :--- | :--- | :--- | :--- | :--- |
| Per-class range | 0.94â€“0.97 | 0.92â€“0.97 | 0.93â€“0.96 | â€“ |
| **Macro & Weighted Avg** | **0.95** | **0.95** | **0.95** | **0.95** |

* The Mistral-7B model with PEFT/LoRA demonstrated **consistent high performance** across all classes, achieving an **Overall Accuracy of 0.95**.<br>
    *(MÃ´ hÃ¬nh Mistral-7B vá»›i PEFT/LoRA thá»ƒ hiá»‡n **hiá»‡u suáº¥t cao nháº¥t quÃ¡n** trÃªn táº¥t cáº£ cÃ¡c lá»›p, Ä‘áº¡t **Äá»™ chÃ­nh xÃ¡c tá»•ng thá»ƒ 0.95**.)*
* Classes such as 8 and 3 achieved the highest Precision and Recall (around 0.97).<br>
    *(CÃ¡c lá»›p nhÆ° 8 vÃ  3 Ä‘áº¡t Precision vÃ  Recall cao nháº¥t (khoáº£ng 0.97).)*

---

## ğŸ Conclusion *(Káº¿t luáº­n)*

* **Power of Transformers:** The fine-tuned Mistral-7B model demonstrated **superior representational power** and deeper contextual understanding compared to classical models. <br>
    *(**Sá»©c máº¡nh cá»§a Transformer:** MÃ´ hÃ¬nh Mistral-7B tinh chá»‰nh Ä‘Ã£ chá»©ng minh **kháº£ nÄƒng biá»ƒu diá»…n vÆ°á»£t trá»™i** vÃ  hiá»ƒu biáº¿t ngá»¯ cáº£nh sÃ¢u sáº¯c hÆ¡n so vá»›i cÃ¡c mÃ´ hÃ¬nh cá»• Ä‘iá»ƒn.)*
* **Efficacy of PEFT:** The use of **LoRA and 4-bit quantization** confirmed that PEFT provides an effective balance between **high performance and low computational cost** for large-scale repository analysis. This method enables the application of high-capacity Transformers to specialized classification problems without full fine-tuning. <br>
    *(**Hiá»‡u quáº£ cá»§a PEFT:** Viá»‡c sá»­ dá»¥ng **LoRA vÃ  lÆ°á»£ng tá»­ hÃ³a 4-bit** Ä‘Ã£ xÃ¡c nháº­n ráº±ng PEFT cung cáº¥p sá»± cÃ¢n báº±ng hiá»‡u quáº£ giá»¯a **hiá»‡u suáº¥t cao vÃ  chi phÃ­ tÃ­nh toÃ¡n tháº¥p** cho phÃ¢n tÃ­ch kho lÆ°u trá»¯ quy mÃ´ lá»›n. PhÆ°Æ¡ng phÃ¡p nÃ y giÃºp Ã¡p dá»¥ng cÃ¡c Transformer cÃ´ng suáº¥t cao vÃ o cÃ¡c váº¥n Ä‘á» phÃ¢n loáº¡i chuyÃªn biá»‡t mÃ  khÃ´ng cáº§n tinh chá»‰nh toÃ n bá»™.)*

---

## ğŸ‘¥ Authors *(NhÃ³m Thá»±c hiá»‡n)*

**Students:** *(Sinh viÃªn thá»±c hiá»‡n)*  
- Há»“ Gia ThÃ nh  
- Huá»³nh ThÃ¡i Linh  
- TrÆ°Æ¡ng Minh Khoa  

**Supervisor:** *(Giáº£ng viÃªn hÆ°á»›ng dáº«n)* *ThS. LÃª Nháº­t TÃ¹ng*  
**University:** *(TrÆ°á»ng)* TrÆ°á»ng Äáº¡i há»c CÃ´ng nghá»‡ TP. Há»“ ChÃ­ Minh â€” *Khoa há»c Dá»¯ liá»‡u*  
**Year:** *(NÄƒm thá»±c hiá»‡n)* 2025

---

> Â© 2025 â€” Project: *GitHub Repository Topic Classification Based on Textual Metadata*  
> *Developed for academic research and educational purposes.*
