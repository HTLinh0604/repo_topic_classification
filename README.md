# ğŸ”¬ PROJECT 2B: GitHub Repository Topic Classification Based on Textual Metadata
*(Dá»° ÃN 2B: PhÃ¢n loáº¡i Chá»§ Ä‘á» Kho lÆ°u trá»¯ GitHub Dá»±a trÃªn SiÃªu dá»¯ liá»‡u VÄƒn báº£n)*

![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Transformers](https://img.shields.io/badge/Transformers-F08030?style=for-the-badge&logo=huggingface&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)
![PEFT](https://img.shields.io/badge/PEFT-FFB762?style=for-the-badge&logo=huggingface&logoColor=black)
![NLTK](https://img.shields.io/badge/NLTK-306998?style=for-the-badge&logo=nltk&logoColor=white)
![Scikit-learn](https://img.shields.io/badge/scikit--learn-F7931E?style=for-the-badge&logo=scikit-learn&logoColor=white)
![GraphQL](https://img.shields.io/badge/GraphQL-E10098?style=for-the-badge&logo=graphql&logoColor=white)

---

## ğŸ¯ Introduction & Goal
*(Giá»›i thiá»‡u & Má»¥c tiÃªu)*

* **Problem:** The rapid growth of open-source (OSS) repositories on GitHub has generated vast amounts of textual metadata. While GitHub allows users to assign topics, these labels are often **inconsistent, incomplete, and user-generated**.
    *(**Váº¥n Ä‘á»:** Sá»± phÃ¡t triá»ƒn nhanh chÃ³ng cá»§a cÃ¡c kho mÃ£ nguá»“n má»Ÿ (OSS) trÃªn GitHub Ä‘Ã£ táº¡o ra lÆ°á»£ng lá»›n siÃªu dá»¯ liá»‡u vÄƒn báº£n. Máº·c dÃ¹ GitHub cho phÃ©p ngÆ°á»i dÃ¹ng gÃ¡n chá»§ Ä‘á», cÃ¡c nhÃ£n nÃ y thÆ°á»ng **khÃ´ng nháº¥t quÃ¡n, khÃ´ng Ä‘áº§y Ä‘á»§ vÃ  do ngÆ°á»i dÃ¹ng táº¡o**.)*

* **Goal:** To develop an automated and comprehensive approach to classify repositories into specific topics based on their textual metadata, primarily the content from **`README.md`** files.
    *(**Má»¥c tiÃªu:** PhÃ¡t triá»ƒn má»™t cÃ¡ch tiáº¿p cáº­n tá»± Ä‘á»™ng vÃ  toÃ n diá»‡n Ä‘á»ƒ phÃ¢n loáº¡i cÃ¡c kho lÆ°u trá»¯ vÃ o cÃ¡c chá»§ Ä‘á» cá»¥ thá»ƒ dá»±a trÃªn siÃªu dá»¯ liá»‡u vÄƒn báº£n, chá»§ yáº¿u lÃ  ná»™i dung tá»« file **`README.md`**.)*

* **Main Contributions:**
    *(**ÄÃ³ng gÃ³p chÃ­nh:**)*
    1.  Constructed and publicly described a **large, diverse dataset** of `README.md` files, covering over **fifty different topics**.
        *(XÃ¢y dá»±ng vÃ  mÃ´ táº£ cÃ´ng khai má»™t **táº­p dá»¯ liá»‡u lá»›n, Ä‘a dáº¡ng** gá»“m cÃ¡c file `README.md`, bao gá»“m hÆ¡n **nÄƒm mÆ°Æ¡i chá»§ Ä‘á»** khÃ¡c nhau.)*
    2.  Developed a **robust text preprocessing** pipeline tailored for software documentation (e.g., stopword enrichment with domain-specific tokens).
        *(PhÃ¡t triá»ƒn quy trÃ¬nh **tiá»n xá»­ lÃ½ vÄƒn báº£n máº¡nh máº½** Ä‘Æ°á»£c Ä‘iá»u chá»‰nh cho tÃ i liá»‡u pháº§n má»m (vÃ­ dá»¥: lÃ m giÃ u stopword báº±ng cÃ¡c mÃ£ thÃ´ng bÃ¡o dÃ nh riÃªng cho miá»n).)*
    3.  Applied and evaluated state-of-the-art Transformer-based models, leveraging **PEFT (LoRA)** to achieve high performance and computational efficiency.
        *(Ãp dá»¥ng vÃ  Ä‘Ã¡nh giÃ¡ cÃ¡c mÃ´ hÃ¬nh dá»±a trÃªn Transformer hiá»‡n Ä‘áº¡i, táº­n dá»¥ng **PEFT (LoRA)** Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t cao vÃ  hiá»‡u quáº£ tÃ­nh toÃ¡n.)*

---

## ğŸ“Š Data & Preprocessing
*(Dá»¯ liá»‡u & Tiá»n xá»­ lÃ½)*

### A. Data Collection
*(A. Thu tháº­p Dá»¯ liá»‡u)*

* **Source:** Used the **GitHub GraphQL API** for data collection.
    *(**Nguá»“n:** Sá»­ dá»¥ng **GitHub GraphQL API** Ä‘á»ƒ thu tháº­p dá»¯ liá»‡u.)*
* **Scale:** The final dataset contains **57,368 `README.md` files**.
    *(**Quy mÃ´:** Táº­p dá»¯ liá»‡u cuá»‘i cÃ¹ng chá»©a **57.368 file `README.md`**.)*
* **Topic Scope:** Over 50 distinct topics within the Information Technology (IT) domain.
    *(**Pháº¡m vi Chá»§ Ä‘á»:** HÆ¡n 50 chá»§ Ä‘á» riÃªng biá»‡t trong lÄ©nh vá»±c CÃ´ng nghá»‡ ThÃ´ng tin (IT).)*
* **Sampling Strategy (to ensure diversity):** For each topic, up to 2,000 repositories were collected based on 4 complementary criteria (approx. 500 repos each):
    *(**Chiáº¿n lÆ°á»£c láº¥y máº«u (Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh Ä‘a dáº¡ng):** Äá»‘i vá»›i má»—i chá»§ Ä‘á», thu tháº­p tá»›i 2.000 kho lÆ°u trá»¯ dá»±a trÃªn 4 tiÃªu chÃ­ bá»• sung (má»—i tiÃªu chÃ­ khoáº£ng 500 repo):)*
    1.  **Most Starred**
        *(**Most Starred** (ÄÆ°á»£c gáº¯n sao nhiá»u nháº¥t).)*
    2.  **Most Forked**
        *(**Most Forked** (ÄÆ°á»£c fork nhiá»u nháº¥t).)*
    3.  **Recently Updated**
        *(**Recently Updated** (ÄÆ°á»£c cáº­p nháº­t gáº§n Ä‘Ã¢y).)*
    4.  **Best Match** (Random sampling for diversity)
        *(**Best Match** (Máº«u ngáº«u nhiÃªn Ä‘á»ƒ tÄƒng tÃ­nh Ä‘a dáº¡ng).)*
* **Target Labels:** 50 granular topics were mapped to **10 broader categories** representing major IT domains, serving as the target labels for the multi-class classification task.
    *(**PhÃ¢n loáº¡i Má»¥c tiÃªu (Target Labels):** 50 chá»§ Ä‘á» chi tiáº¿t Ä‘Ã£ Ä‘Æ°á»£c Ã¡nh xáº¡ thÃ nh **10 danh má»¥c rá»™ng hÆ¡n** (categories) Ä‘áº¡i diá»‡n cho cÃ¡c miá»n chÃ­nh trong IT, dÃ¹ng lÃ m nhÃ£n má»¥c tiÃªu cho nhiá»‡m vá»¥ phÃ¢n loáº¡i Ä‘a lá»›p.)*
* **Data Distribution:** The dataset was split using **stratified sampling** with an **80% Train** and **20% Test** ratio. Total 57,368 samples, with 45,894 training and 11,474 test samples.
    *(**PhÃ¢n phá»‘i Dá»¯ liá»‡u:** Táº­p dá»¯ liá»‡u Ä‘Æ°á»£c phÃ¢n chia theo **láº¥y máº«u phÃ¢n táº§ng (stratified sampling)** vá»›i tá»· lá»‡ **80% cho táº­p huáº¥n luyá»‡n (Train)** vÃ  **20% cho táº­p kiá»ƒm tra (Test)**. Tá»•ng cá»™ng 57.368 máº«u, vá»›i 45.894 máº«u huáº¥n luyá»‡n vÃ  11.474 máº«u kiá»ƒm tra.)*

### B. Preprocessing Pipeline
*(B. Quy trÃ¬nh Tiá»n xá»­ lÃ½)*

A custom preprocessing pipeline was applied to remove noise typical in software documentation:
*(Má»™t pipeline tiá»n xá»­ lÃ½ tÃ¹y chá»‰nh Ä‘Ã£ Ä‘Æ°á»£c Ã¡p dá»¥ng Ä‘á»ƒ loáº¡i bá» nhiá»…u Ä‘iá»ƒn hÃ¬nh trong tÃ i liá»‡u pháº§n má»m:)*

1.  **Code and URL Removal:** Stripped Markdown code blocks, inline code snippets, and hyperlinks.
    *(**Loáº¡i bá» Code vÃ  URL:** XÃ³a cÃ¡c khá»‘i mÃ£ Markdown, Ä‘oáº¡n mÃ£ ná»™i tuyáº¿n vÃ  siÃªu liÃªn káº¿t.)*
2.  **Markdown Syntax Cleaning:** Normalized headers, bold/italic formatting to plain text.
    *(**LÃ m sáº¡ch CÃº phÃ¡p Markdown:** Chuáº©n hÃ³a cÃ¡c dáº¥u tiÃªu Ä‘á», Ä‘á»‹nh dáº¡ng in Ä‘áº­m/in nghiÃªng thÃ nh vÄƒn báº£n thuáº§n tÃºY.)*
3.  **Tokenization and Lemmatization:** Used `NLTK` to reduce word inflections.
    *(**Tokenization vÃ  Lemmatization:** Sá»­ dá»¥ng `NLTK` Ä‘á»ƒ giáº£m cÃ¡c dáº¡ng biáº¿n tá»‘ cá»§a tá»«.)*
4.  **Stopword Filtering:** Extended the standard English stopword list with a custom list of programming-related terms (e.g., *install, build, repository, command, example, file*) to reduce uninformative content.
    *(**Lá»c Stopword:** Má»Ÿ rá»™ng danh sÃ¡ch stopword tiáº¿ng Anh tiÃªu chuáº©n báº±ng má»™t danh sÃ¡ch cÃ¡c thuáº­t ngá»¯ liÃªn quan Ä‘áº¿n láº­p trÃ¬nh (vÃ­ dá»¥: *install, build, repository, command, example, file*) Ä‘á»ƒ giáº£m thiá»ƒu ná»™i dung khÃ´ng mang thÃ´ng tin.)*
5.  **Lowercasing and Noise Removal:** All text was lowercased and cleaned of punctuation, digits, and special characters.
    *(**Chuyá»ƒn vá» chá»¯ thÆ°á»ng vÃ  loáº¡i bá» nhiá»…u:** Táº¥t cáº£ vÄƒn báº£n Ä‘Æ°á»£c chuyá»ƒn vá» chá»¯ thÆ°á»ng vÃ  lÃ m sáº¡ch khá»i dáº¥u cÃ¢u, chá»¯ sá»‘ vÃ  kÃ½ tá»± Ä‘áº·c biá»‡t.)*

---

## ğŸ› ï¸ Methodology & Models
*(PhÆ°Æ¡ng phÃ¡p & Kiáº¿n trÃºc MÃ´ hÃ¬nh)*

### A. Feature Representation
*(A. Biá»ƒu diá»…n Äáº·c trÆ°ng)*

* **For Classical Models:** Utilized **Sentence-BERT** embeddings (specifically the **all-MiniLM-L6-v2** model) to convert each preprocessed README into a dense **384-dimensional** feature vector.
    *(**Äá»‘i vá»›i MÃ´ hÃ¬nh Cá»• Ä‘iá»ƒn:** Sá»­ dá»¥ng nhÃºng cÃ¢u **Sentence-BERT** (cá»¥ thá»ƒ lÃ  mÃ´ hÃ¬nh **all-MiniLM-L6-v2**) Ä‘á»ƒ chuyá»ƒn Ä‘á»•i má»—i file README Ä‘Ã£ Ä‘Æ°á»£c tiá»n xá»­ lÃ½ thÃ nh má»™t vector Ä‘áº·c trÆ°ng dÃ y Ä‘áº·c **384 chiá»u**.)*
* **For Transformer Models:** Used the **AutoTokenizer** corresponding to the base model **Mistral-7B-v0.1**. Each text was encoded into a fixed-length sequence of **512 tokens**.
    *(**Äá»‘i vá»›i MÃ´ hÃ¬nh Transformer:** Sá»­ dá»¥ng **AutoTokenizer** tÆ°Æ¡ng á»©ng vá»›i mÃ´ hÃ¬nh cÆ¡ sá»Ÿ **Mistral-7B-v0.1**. Má»—i vÄƒn báº£n Ä‘Æ°á»£c mÃ£ hÃ³a thÃ nh má»™t chuá»—i ID token cÃ³ Ä‘á»™ dÃ i cá»‘ Ä‘á»‹nh **512 tokens**.)*

### B. Models Compared
*(B. MÃ´ hÃ¬nh Ä‘Æ°á»£c So sÃ¡nh)*

1.  **Classical Machine Learning Models:** Trained on MiniLM embeddings:
    *(**MÃ´ hÃ¬nh Há»c mÃ¡y Cá»• Ä‘iá»ƒn:** Huáº¥n luyá»‡n trÃªn cÃ¡c nhÃºng MiniLM:)*
    * Logistic Regression (LR)
    * Random Forest (RF)
    * Support Vector Classifier (SVC) (using a linear kernel)
    * K-Nearest Neighbors (KNN)
2.  **Modern Transformer Model (PEFT):**
    *(**MÃ´ hÃ¬nh Transformer Hiá»‡n Ä‘áº¡i (PEFT):**)*
    * **Mistral-7B** fine-tuned using **Low-Rank Adaptation (LoRA)** and **4-bit Quantization**.
        *(**Mistral-7B** Ä‘Æ°á»£c tinh chá»‰nh báº±ng cÃ¡ch sá»­ dá»¥ng **Low-Rank Adaptation (LoRA)** vÃ  **LÆ°á»£ng tá»­ hÃ³a 4-bit**.)*
    * The PEFT/LoRA technique significantly reduced trainable parameters (using a low rank $r=16$) while maintaining robust model performance.
        *(Ká»¹ thuáº­t PEFT/LoRA giáº£m Ä‘Ã¡ng ká»ƒ cÃ¡c tham sá»‘ huáº¥n luyá»‡n (sá»­ dá»¥ng chiá»u rank tháº¥p $r=16$) trong khi váº«n duy trÃ¬ hiá»‡u suáº¥t mÃ´ hÃ¬nh máº¡nh máº½.)*

---

## ğŸ“ˆ Experimental Results
*(Káº¿t quáº£ Thá»±c nghiá»‡m)*

Performance was evaluated using Precision (P), Recall (R), F1-score (F1), and Accuracy (A).
*(Hiá»‡u suáº¥t Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ báº±ng Precision (P), Recall (R), F1-score (F1), vÃ  Accuracy (A).)*

### A. Classical Model Performance
*(A. Hiá»‡u suáº¥t MÃ´ hÃ¬nh Cá»• Ä‘iá»ƒn)*

The classical machine learning models achieved **moderate results**, indicating a limited ability to capture complex semantic context.
*(CÃ¡c mÃ´ hÃ¬nh há»c mÃ¡y cá»• Ä‘iá»ƒn Ä‘áº¡t káº¿t quáº£ **á»Ÿ má»©c vá»«a pháº£i**, cho tháº¥y kháº£ nÄƒng háº¡n cháº¿ trong viá»‡c náº¯m báº¯t bá»‘i cáº£nh ngá»¯ nghÄ©a phá»©c táº¡p.)*

| Model | Precision | Recall | **F1-Score** |
| :--- | :--- | :--- | :--- |
| Logistic Regression | 0.66 | 0.69 | 0.66 |
| Random Forest | 0.58 | 0.62 | **0.56** (Lowest) |
| SVC | 0.66 | 0.69 | 0.67 |
| **KNN** | 0.67 | 0.70 | **0.68** (Highest in classical) |

### B. Transformer Model Performance (Mistral-7B + PEFT/LoRA)
*(B. Hiá»‡u suáº¥t MÃ´ hÃ¬nh Transformer (Mistral-7B + PEFT/LoRA))*

The fine-tuned Transformer model achieved **superior performance improvements**.
*(MÃ´ hÃ¬nh Transformer Ä‘Æ°á»£c tinh chá»‰nh Ä‘áº¡t Ä‘Æ°á»£c sá»± **cáº£i thiá»‡n hiá»‡u suáº¥t vÆ°á»£t trá»™i**.)*

| Metric Type | Precision | Recall | **F1-score** | **Accuracy** |
| :--- | :--- | :--- | :--- | :--- |
| Per-class range | 0.94â€“0.97 | 0.92â€“0.97 | 0.93â€“0.96 | â€“ |
| **Macro & Weighted Avg** | **0.95** | **0.95** | **0.95** | **0.95** |

* The Mistral-7B model with PEFT/LoRA demonstrated **consistent high performance** across all classes, achieving an **Overall Accuracy of 0.95**.
    *(MÃ´ hÃ¬nh Mistral-7B vá»›i PEFT/LoRA thá»ƒ hiá»‡n **hiá»‡u suáº¥t cao nháº¥t quÃ¡n** trÃªn táº¥t cáº£ cÃ¡c lá»›p, Ä‘áº¡t **Äá»™ chÃ­nh xÃ¡c tá»•ng thá»ƒ 0.95**.)*
* Classes such as 8 and 3 achieved the highest Precision and Recall (around 0.97).
    *(CÃ¡c lá»›p nhÆ° 8 vÃ  3 Ä‘áº¡t Precision vÃ  Recall cao nháº¥t (khoáº£ng 0.97).)*

---

## ğŸ Conclusion
*(Káº¿t luáº­n)*

* **Power of Transformers:** The fine-tuned Mistral-7B model demonstrated **superior representational power** and deeper contextual understanding compared to classical models.
    *(**Sá»©c máº¡nh cá»§a Transformer:** MÃ´ hÃ¬nh Mistral-7B tinh chá»‰nh Ä‘Ã£ chá»©ng minh **kháº£ nÄƒng biá»ƒu diá»…n vÆ°á»£t trá»™i** vÃ  hiá»ƒu biáº¿t ngá»¯ cáº£nh sÃ¢u sáº¯c hÆ¡n so vá»›i cÃ¡c mÃ´ hÃ¬nh cá»• Ä‘iá»ƒn.)*
* **Efficacy of PEFT:** The use of **LoRA and 4-bit quantization** confirmed that PEFT provides an effective balance between **high performance and low computational cost** for large-scale repository analysis. This method enables the application of high-capacity Transformers to specialized classification problems without full fine-tuning.
    *(**Hiá»‡u quáº£ cá»§a PEFT:** Viá»‡c sá»­ dá»¥ng **LoRA vÃ  lÆ°á»£ng tá»­ hÃ³a 4-bit** Ä‘Ã£ xÃ¡c nháº­n ráº±ng PEFT cung cáº¥p sá»± cÃ¢n báº±ng hiá»‡u quáº£ giá»¯a **hiá»‡u suáº¥t cao vÃ  chi phÃ­ tÃ­nh toÃ¡n tháº¥p** cho phÃ¢n tÃ­ch kho lÆ°u trá»¯ quy mÃ´ lá»›n. PhÆ°Æ¡ng phÃ¡p nÃ y giÃºp Ã¡p dá»¥ng cÃ¡c Transformer cÃ´ng suáº¥t cao vÃ o cÃ¡c váº¥n Ä‘á» phÃ¢n loáº¡i chuyÃªn biá»‡t mÃ  khÃ´ng cáº§n tinh chá»‰nh toÃ n bá»™.)*

---

### ğŸ’¡ Analogy
*(PhÃ©p TÆ°Æ¡ng tá»±)*

> You can think of classifying GitHub repository topics like organizing a massive library. Classical models (LR, KNN) are like trying to sort books based only on their length or the frequency of a few keywords (TF-IDF/MiniLM)â€”you might separate sci-fi from cookbooks, but you'll struggle with complex novels (overlapping topics).
>
> *(Báº¡n cÃ³ thá»ƒ coi viá»‡c phÃ¢n loáº¡i chá»§ Ä‘á» kho lÆ°u trá»¯ GitHub giá»‘ng nhÆ° viá»‡c sáº¯p xáº¿p má»™t thÆ° viá»‡n khá»•ng lá»“. CÃ¡c mÃ´ hÃ¬nh cá»• Ä‘iá»ƒn (LR, KNN) giá»‘ng nhÆ° viá»‡c báº¡n cá»‘ gáº¯ng sáº¯p xáº¿p sÃ¡ch chá»‰ dá»±a trÃªn Ä‘á»™ dÃ i hoáº·c táº§n suáº¥t xuáº¥t hiá»‡n cá»§a má»™t vÃ i tá»« khÃ³a (TF-IDF/MiniLM)â€”báº¡n cÃ³ thá»ƒ phÃ¢n loáº¡i Ä‘Æ°á»£c sÃ¡ch khoa há»c viá»…n tÆ°á»Ÿng vÃ  sÃ¡ch dáº¡y náº¥u Äƒn, nhÆ°ng sáº½ gáº·p khÃ³ khÄƒn vá»›i cÃ¡c tiá»ƒu thuyáº¿t phá»©c táº¡p (chá»§ Ä‘á» chá»“ng chÃ©o).)*
>
> In contrast, the PEFT-tuned Transformer model is like a **highly trained librarian (Mistral-7B)** who doesn't just read keywords but understands the context, sentence structure, and deep purpose of the book (the README), allowing them to accurately shelve every topic, even the most specialized or hybrid ones, with high speed and efficiency.
>
> *(NgÆ°á»£c láº¡i, mÃ´ hÃ¬nh Transformer Ä‘Æ°á»£c tinh chá»‰nh báº±ng PEFT giá»‘ng nhÆ° má»™t **thá»§ thÆ° Ä‘Æ°á»£c Ä‘Ã o táº¡o chuyÃªn sÃ¢u (Mistral-7B)**, ngÆ°á»i khÃ´ng chá»‰ Ä‘á»c tá»« khÃ³a mÃ  cÃ²n hiá»ƒu Ä‘Æ°á»£c bá»‘i cáº£nh, cáº¥u trÃºc cÃ¢u vÃ  má»¥c Ä‘Ã­ch sÃ¢u xa cá»§a cuá»‘n sÃ¡ch (tÃ i liá»‡u README), cho phÃ©p há» sáº¯p xáº¿p chÃ­nh xÃ¡c má»i chá»§ Ä‘á», ká»ƒ cáº£ nhá»¯ng cuá»‘n sÃ¡ch chuyÃªn biá»‡t hoáº·c lai táº¡p nháº¥t, vá»›i tá»‘c Ä‘á»™ vÃ  hiá»‡u suáº¥t cao.)*
