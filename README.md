# ğŸš€ PROJECT 2B: GitHub Repository Topic Classification Based on Textual Metadata  
*(Dá»± Ã¡n 2B: PhÃ¢n loáº¡i chá»§ Ä‘á» kho GitHub dá»±a trÃªn siÃªu dá»¯ liá»‡u vÄƒn báº£n)*  

**ğŸ§  Technologies:** Python Â· PyTorch Â· HuggingFace Transformers Â· PEFT (LoRA) Â· Scikit-learn Â· NLTK Â· GitHub GraphQL API  

---

## ğŸ§© 1. Introduction & Goal  
*(Giá»›i thiá»‡u vÃ  Má»¥c tiÃªu Dá»± Ã¡n)*  

**Project Name:** PROJECT 2B: GitHub Repository Topic Classification Based on Textual Metadata.  
*(TÃªn Dá»± Ã¡n: PhÃ¢n loáº¡i chá»§ Ä‘á» kho GitHub dá»±a trÃªn siÃªu dá»¯ liá»‡u vÄƒn báº£n.)*  

**Problem:** The rapid growth of open-source repositories on GitHub has generated vast textual metadata, but user-created topic tags are often **inconsistent, incomplete, and unreliable**.  
*(Váº¥n Ä‘á»: Sá»± phÃ¡t triá»ƒn nhanh cá»§a kho mÃ£ nguá»“n má»Ÿ GitHub táº¡o ra lÆ°á»£ng lá»›n siÃªu dá»¯ liá»‡u vÄƒn báº£n, nhÆ°ng nhÃ£n chá»§ Ä‘á» thÆ°á»ng **khÃ´ng nháº¥t quÃ¡n, khÃ´ng Ä‘áº§y Ä‘á»§ vÃ  phá»¥ thuá»™c ngÆ°á»i dÃ¹ng**.)*  

**Goal:** Develop an **automated and robust classification approach** based on textual metadata, primarily extracted from **`README.md`** files.  
*(Má»¥c tiÃªu: PhÃ¡t triá»ƒn phÆ°Æ¡ng phÃ¡p **phÃ¢n loáº¡i tá»± Ä‘á»™ng vÃ  máº¡nh máº½** dá»±a trÃªn siÃªu dá»¯ liá»‡u vÄƒn báº£n, chá»§ yáº¿u tá»« file **`README.md`**.)*  

**Main Contributions:**  
*(ÄÃ³ng gÃ³p chÃ­nh:)*  
1ï¸âƒ£ Build a **large, diverse public dataset** with over **50 unique topics** from README.md files.  
*(XÃ¢y dá»±ng vÃ  cÃ´ng khai **táº­p dá»¯ liá»‡u lá»›n, Ä‘a dáº¡ng** gá»“m hÆ¡n **50 chá»§ Ä‘á»** khÃ¡c nhau.)*  
2ï¸âƒ£ Develop a **domain-specific text preprocessing pipeline**, enriching stopwords with software-related tokens.  
*(PhÃ¡t triá»ƒn **pipeline tiá»n xá»­ lÃ½ Ä‘áº·c thÃ¹ miá»n**, má»Ÿ rá»™ng stopword báº±ng cÃ¡c tá»« liÃªn quan Ä‘áº¿n láº­p trÃ¬nh.)*  
3ï¸âƒ£ Apply **Transformer-based models using PEFT (LoRA)** for high accuracy and efficiency.  
*(Ãp dá»¥ng **mÃ´ hÃ¬nh Transformer vá»›i PEFT (LoRA)** nháº±m Ä‘áº¡t hiá»‡u suáº¥t cao vÃ  tiáº¿t kiá»‡m tÃ i nguyÃªn.)*  

---

## ğŸ“š 2. Data & Preprocessing  
*(Dá»¯ liá»‡u vÃ  Tiá»n xá»­ lÃ½)*  

### A. Data Collection  
*(Thu tháº­p Dá»¯ liá»‡u)*  

**Source:** Collected via **GitHub GraphQL API**.  
*(Nguá»“n: Thu tháº­p qua GitHub GraphQL API.)*  

**Scale:** Contains **57,368 README.md files**.  
*(Quy mÃ´: Gá»“m 57.368 file README.md.)*  

**Topic Coverage:** Over **50 distinct IT-related topics**.  
*(Pháº¡m vi: HÆ¡n 50 chá»§ Ä‘á» thuá»™c lÄ©nh vá»±c CNTT.)*  

**Sampling Strategy:** For each topic, up to 2,000 repositories were selected based on:  
*(Chiáº¿n lÆ°á»£c láº¥y máº«u: Má»—i chá»§ Ä‘á» thu tháº­p tá»‘i Ä‘a 2.000 kho dá»±a trÃªn cÃ¡c tiÃªu chÃ­:)*  
1ï¸âƒ£ Most Starred  
*(ÄÆ°á»£c gáº¯n sao nhiá»u nháº¥t)*  
2ï¸âƒ£ Most Forked  
*(ÄÆ°á»£c fork nhiá»u nháº¥t)*  
3ï¸âƒ£ Recently Updated  
*(Cáº­p nháº­t gáº§n Ä‘Ã¢y nháº¥t)*  
4ï¸âƒ£ Best Match (random sampling)  
*(Máº«u ngáº«u nhiÃªn Ä‘á»ƒ tÄƒng tÃ­nh Ä‘a dáº¡ng)*  

**Target Labels:** 50 topics mapped into **10 broader IT categories** for multi-class classification.  
*(PhÃ¢n loáº¡i Má»¥c tiÃªu: 50 chá»§ Ä‘á» Ã¡nh xáº¡ thÃ nh 10 danh má»¥c chÃ­nh trong IT Ä‘á»ƒ phá»¥c vá»¥ bÃ i toÃ¡n phÃ¢n loáº¡i Ä‘a lá»›p.)*  

**Data Split:** **80% training / 20% testing**, stratified by topic (45,894 train, 11,474 test).  
*(PhÃ¢n phá»‘i Dá»¯ liá»‡u: 80% huáº¥n luyá»‡n, 20% kiá»ƒm tra, chia theo phÆ°Æ¡ng phÃ¡p phÃ¢n táº§ng.)*  

---

### B. Preprocessing Pipeline  
*(Quy trÃ¬nh Tiá»n xá»­ lÃ½)*  

1ï¸âƒ£ **Remove Code & URLs:** Clean Markdown code blocks, inline code, and hyperlinks.  
*(XÃ³a cÃ¡c khá»‘i code, mÃ£ ná»™i tuyáº¿n vÃ  liÃªn káº¿t.)*  

2ï¸âƒ£ **Normalize Markdown Syntax:** Convert formatting to plain text.  
*(Chuáº©n hÃ³a cÃº phÃ¡p Markdown vá» dáº¡ng vÄƒn báº£n thuáº§n.)*  

3ï¸âƒ£ **Tokenization & Lemmatization:** Performed using `NLTK`.  
*(Thá»±c hiá»‡n tÃ¡ch vÃ  chuáº©n hÃ³a tá»« báº±ng NLTK.)*  

4ï¸âƒ£ **Custom Stopwords:** Extend with programming-related terms (*install, build, repository, file*...).  
*(Má»Ÿ rá»™ng danh sÃ¡ch stopword báº±ng cÃ¡c thuáº­t ngá»¯ láº­p trÃ¬nh.)*  

5ï¸âƒ£ **Lowercasing & Noise Removal:** Remove digits, punctuation, and special symbols.  
*(Chuyá»ƒn toÃ n bá»™ vá» chá»¯ thÆ°á»ng vÃ  loáº¡i bá» kÃ½ tá»± nhiá»…u, sá»‘, dáº¥u cÃ¢u.)*  

---

## âš™ï¸ 3. Methodology & Models  
*(PhÆ°Æ¡ng phÃ¡p vÃ  Kiáº¿n trÃºc MÃ´ hÃ¬nh)*  

### A. Feature Representation  
*(Biá»ƒu diá»…n Äáº·c trÆ°ng)*  

**Classical Models:** Use **Sentence-BERT embeddings (all-MiniLM-L6-v2)** to convert text into 384-dimensional dense vectors.  
*(MÃ´ hÃ¬nh cá»• Ä‘iá»ƒn: Sá»­ dá»¥ng Sentence-BERT (all-MiniLM-L6-v2) Ä‘á»ƒ biá»ƒu diá»…n vÄƒn báº£n thÃ nh vector 384 chiá»u.)*  

**Transformer Models:** Use **AutoTokenizer (Mistral-7B-v0.1)** with a fixed input length of **512 tokens**.  
*(MÃ´ hÃ¬nh Transformer: Sá»­ dá»¥ng AutoTokenizer (Mistral-7B-v0.1) vá»›i Ä‘á»™ dÃ i Ä‘áº§u vÃ o cá»‘ Ä‘á»‹nh 512 tokens.)*  

---

### B. Models Compared  
*(CÃ¡c MÃ´ hÃ¬nh So sÃ¡nh)*  

**Classical Machine Learning Models:** (trained on MiniLM embeddings)  
*(MÃ´ hÃ¬nh Há»c mÃ¡y Cá»• Ä‘iá»ƒn: huáº¥n luyá»‡n trÃªn nhÃºng MiniLM)*  
- Logistic Regression (LR)  
- Random Forest (RF)  
- Support Vector Classifier (SVC)  
- K-Nearest Neighbors (KNN)  

**Modern Transformer (PEFT):**  
*(MÃ´ hÃ¬nh Transformer Hiá»‡n Ä‘áº¡i vá»›i PEFT)*  
- **Mistral-7B** fine-tuned with **Low-Rank Adaptation (LoRA)** and **4-bit quantization**.  
*(Mistral-7B Ä‘Æ°á»£c tinh chá»‰nh báº±ng LoRA vÃ  lÆ°á»£ng tá»­ hÃ³a 4-bit.)*  
- Reduces trainable parameters ($r=16$) while maintaining performance.  
*(Giáº£m tham sá»‘ huáº¥n luyá»‡n Ä‘Ã¡ng ká»ƒ nhÆ°ng váº«n Ä‘áº£m báº£o hiá»‡u suáº¥t cao.)*  

---

## ğŸ“Š 4. Experimental Results  
*(Káº¿t quáº£ Thá»±c nghiá»‡m)*  

**Metrics:** Precision (P), Recall (R), F1-score (F1), and Accuracy (A).  
*(Chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡: Precision, Recall, F1-score, Accuracy.)*  

### A. Classical Models Performance  
*(Hiá»‡u suáº¥t MÃ´ hÃ¬nh Cá»• Ä‘iá»ƒn)*  

| Model | Precision | Recall | **F1-Score** |  
| :--- | :--- | :--- | :--- |  
| Logistic Regression | 0.66 | 0.69 | 0.66 |  
| Random Forest | 0.58 | 0.62 | **0.56** (Lowest) |  
| SVC | 0.66 | 0.69 | 0.67 |  
| **KNN** | 0.67 | 0.70 | **0.68** (Best) |  

*(Nháº­n xÃ©t: CÃ¡c mÃ´ hÃ¬nh há»c mÃ¡y cá»• Ä‘iá»ƒn Ä‘áº¡t káº¿t quáº£ trung bÃ¬nh, háº¡n cháº¿ trong viá»‡c náº¯m báº¯t ngá»¯ nghÄ©a phá»©c táº¡p.)*  

---

### B. Transformer Model (Mistral-7B + PEFT/LoRA)  
*(MÃ´ hÃ¬nh Transformer: Mistral-7B + PEFT/LoRA)*  

| Metric Type | Precision | Recall | **F1-score** | **Accuracy** |  
| :--- | :--- | :--- | :--- | :--- |  
| Per-class Range | 0.94â€“0.97 | 0.92â€“0.97 | 0.93â€“0.96 | â€“ |  
| **Macro / Weighted Avg.** | **0.95** | **0.95** | **0.95** | **0.95** |  

*(Nháº­n xÃ©t: Mistral-7B (PEFT/LoRA) Ä‘áº¡t hiá»‡u suáº¥t cao nháº¥t vá»›i Ä‘á»™ chÃ­nh xÃ¡c 0.95 á»•n Ä‘á»‹nh trÃªn má»i lá»›p.)*  

---

## ğŸ§  5. Conclusion  
*(Káº¿t luáº­n)*  

âœ… **Transformer Strength:** Fine-tuned Mistral-7B exhibits superior contextual understanding compared to traditional ML models.  
*(Mistral-7B tinh chá»‰nh thá»ƒ hiá»‡n kháº£ nÄƒng hiá»ƒu ngá»¯ cáº£nh vÆ°á»£t trá»™i so vá»›i mÃ´ hÃ¬nh cá»• Ä‘iá»ƒn.)*  

âš™ï¸ **Efficiency of PEFT:** LoRA and 4-bit quantization balance **high performance with low computational cost**, making it suitable for large-scale GitHub repository analysis.  
*(LoRA vÃ  lÆ°á»£ng tá»­ hÃ³a 4-bit mang láº¡i hiá»‡u suáº¥t cao vá»›i chi phÃ­ tÃ­nh toÃ¡n tháº¥p, phÃ¹ há»£p cho phÃ¢n tÃ­ch quy mÃ´ lá»›n.)*  

---

## ğŸ“š Analogy  
*(So sÃ¡nh Minh há»a)*  

Classical models (LR, KNN) resemble sorting books by keywords â€” they work for simple distinctions but fail for complex semantics.  
*(MÃ´ hÃ¬nh cá»• Ä‘iá»ƒn giá»‘ng viá»‡c phÃ¢n loáº¡i sÃ¡ch báº±ng tá»« khÃ³a â€” hiá»‡u quáº£ vá»›i chá»§ Ä‘á» Ä‘Æ¡n giáº£n nhÆ°ng kÃ©m vá»›i ngá»¯ nghÄ©a phá»©c táº¡p.)*  

Fine-tuned Transformers like **Mistral-7B + PEFT** act as **expert librarians**, understanding context and semantics deeply to classify even nuanced topics accurately.  
*(Transformer tinh chá»‰nh nhÆ° Mistral-7B + PEFT giá»‘ng nhÆ° thá»§ thÆ° chuyÃªn nghiá»‡p, hiá»ƒu sÃ¢u ngá»¯ nghÄ©a vÃ  phÃ¢n loáº¡i chÃ­nh xÃ¡c cáº£ cÃ¡c chá»§ Ä‘á» phá»©c táº¡p.)*  
